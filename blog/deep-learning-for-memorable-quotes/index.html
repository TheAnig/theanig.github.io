<!doctype html><html data-theme=light lang=en xmlns=http://www.w3.org/1999/xhtml><head><meta charset=UTF-8><meta content="This is my own personal corner of the internet" name=description><meta content="width=device-width,initial-scale=1" name=viewport><meta content=#00bf63 name=theme-color><title>Deep Learning Methods for Quotable Text - TheAnig</title><link href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/ rel=canonical><link href=https://theanig.github.io/favicon.png rel=icon type=image/png><link href=https://theanig.github.io/apple-touch-icon.png rel=apple-touch-icon sizes=180x180 type=image/png><link title="TheAnig - RSS Feed" href=https://theanig.github.io/rss.xml rel=alternate type=application/rss+xml><link title="TheAnig - Atom Feed" href=https://theanig.github.io/atom.xml rel=alternate type=application/atom+xml><style>:root{--accent-color:#00bf63}</style><link href=https://theanig.github.io/style.css rel=stylesheet><link href=https://theanig.github.io/katex.css rel=stylesheet><script defer src=https://theanig.github.io/closable.js></script><script defer src=https://theanig.github.io/copy-button.js></script><script data-goatcounter=https://stats.theanig.dev/count defer src=https://theanig.github.io/count.js></script><script defer src=https://theanig.github.io/katex.min.js></script><script defer src=https://theanig.github.io/auto-render.min.js></script><script defer src=https://theanig.github.io/katex-init.js></script><script defer src=https://theanig.github.io/elasticlunr.min.js></script><script defer src=https://theanig.github.io/search-elasticlunr.js></script><meta content=TheAnig property=og:site_name><meta content="Deep Learning Methods for Quotable Text - TheAnig" property=og:title><meta content=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/ property=og:url><meta content=" The goal of this study is to find out what makes a given sentence more memorable and quotable than others. Traditional methods of linguistic analysis have been done historically and they’ve had limited sucess. The aim of this project is to find out whether a machine learning solely on its own, is able to learn more effectively. Part of Documentation for Social Media and Text Analysis project under Prof.  Wei Xu at The Ohio State University" property=og:description><meta content=https://theanig.github.io/card.png property=og:image><meta content=en_US property=og:locale><body><div id=handle></div><header id=site-nav><nav><a href=#main-content tabindex=0> Skip to Main Content </a><ul><li id=home><a href=https://theanig.github.io> <i class=icon></i>TheAnig</a><li class=divider><li><a href=https://theanig.github.io/blog/>Blog</a><li id=search><button class=circle id=search-toggle title=Search><i class=icon></i></button><li id=theme-switcher><details class=closable><summary class=circle title=Theme><i class=icon></i></summary> <ul><li><button title="Switch to Light Theme" class=circle id=theme-light><i class=icon></i></button><li><button title="Switch to Dark Theme" class=circle id=theme-dark><i class=icon></i></button><li><button title="Use System Theme" class=circle id=theme-system><i class=icon></i></button></ul></details><li id=feed><details class=closable><summary class=circle title=Feed><i class=icon></i></summary> <ul><li><a href=https://theanig.github.io/rss.xml>RSS</a><li><a href=https://theanig.github.io/atom.xml>Atom</a></ul></details></ul></nav><div id=search-container><label class=visually-hidden for=search-bar>Search</label><input placeholder="Search for…" autocomplete=off disabled id=search-bar type=search><div id=search-results-container><div id=search-results></div></div></div></header><main id=main-content><article><div id=heading><p><small> <time datetime=" 2018-07-26T00:00:00+00:00">Published on July 26, 2018</time></small><h1>Deep Learning Methods for Quotable Text</h1><ul class=tags><li><a class=tag href=https://theanig.github.io/tags/memorable-quotes-project/>memorable-quotes-project</a><li><a class=tag href=https://theanig.github.io/tags/deep-learning/>deep-learning</a><li><a class=tag href=https://theanig.github.io/tags/text-classification/>text-classification</a><li><a class=tag href=https://theanig.github.io/tags/natural-language-processing/>natural-language-processing</a></ul></div><div id=buttons-container><details class=closable id=toc><summary title="Table of Contents"><i class=icon></i></summary> <div><strong class=title>Table of Contents</strong><div><ul><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#abstract>Abstract</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#introduction>Introduction</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#literature-review>Literature Review</a><ul><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#text-aesthetics-2014>Text Aesthetics 2014</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#bendersky-2012>Bendersky 2012</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#memorability-2012>Memorability 2012</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#koto-2014>Koto 2014</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#koto-2015>Koto 2015</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#koto-2016>Koto 2016</a></ul><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#method>Method</a><ul><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#textcnn>TextCNN</a></ul><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#experiments>Experiments</a><ul><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#data>Data</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#metrics>Metrics</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#baselines>Baselines</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#performance-and-results-of-the-method>Performance and Results of the method</a></ul><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#conclusions>Conclusions</a><li><a href=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/#replicability>Replicability</a></ul></div></div></details><a title="Go to Top" href=#top id=go-to-top><i class=icon></i></a><a href="https://shareopenly.org/share/?url=https://theanig.github.io/blog/deep-learning-for-memorable-quotes/&text=%20The%20goal%20of%20this%20study%20is%20to%20find%20out%20what%20makes%20a%20given%20sentence%20more%20memorable%20and%20quotable%20than%20others.%20Traditional%20methods%20of%20linguistic%20analysis%20have%20been%20done%20historically%20and%20they%E2%80%99ve%20had%20limited%20sucess.%20The%20aim%20of%20this%20project%20is%20to%20find%20out%20whether%20a%20machine%20learning%20solely%20on%20its%20own%2C%20is%20able%20to%20learn%20more%20effectively.%20Part%20of%20Documentation%20for%20Social%20Media%20and%20Text%20Analysis%20project%20under%20Prof.%20%20Wei%20Xu%20at%20The%20Ohio%20State%20University" id=share title=Share><i class=icon></i></a><a title="File an Issue" href=https://github.com/TheAnig/theanig.github.io/issues id=issue><i class=icon></i></a></div><h2 id=abstract>Abstract</h2><blockquote><p>The goal of this study is to find out what makes a given sentence more memorable and quotable than others. Traditional methods of linguistic analysis have been done historically and they’ve had limited sucess. The aim of this study is to find out whether a machine learning solely on its own, is able to learn more effectively.</blockquote><span id=continue-reading></span><h2 id=introduction>Introduction</h2><p>It is widely known fact that in literature there are some quotes that are more memorable than others. In order to capture this essence of what makes the quote memorable, techniques have been devised based on linguistic expertise to hand pick features that is believed to contribute to making the quote memorable. The limitation of this approach is not only the fact that these generated features scale poorly from dataset to dataset, their performance is also upper-bounded by the quality of the features that are used.<p>In this project, I try to overcome some of these limitations by applying a supervised machine learning technique that ’‘learns’’ what makes the quotes memorable.<h2 id=literature-review>Literature Review</h2><h3 id=text-aesthetics-2014>Text Aesthetics 2014</h3><p><cite>“Automatic prediction of text aesthetics and interestingness” D Ganguly et al (2014)</cite><ul><li>Used statistical features (discussed below)<li>Kindle dataset similar to the one I’ve acquired<li>56% accuracy<li>A comparative study on Naive Bayes, one class SVM, binary SVM<li>One class performed best<li>Used the “mapping convergence” algorithm<li>They apply SVDD to reduce impact of outliers<li>Rely on average length of word, positional difference, POS features include adjectives and adverbs, sentiword net distance, and semantic distance.<li>Used a distance formula extensively, no idea why, only appears in this paper (probably something to do with MC SVM?)</ul><p>$$F(p) = \frac{2}{N*(N-1)}*\sum_{i=1}^N\sum_{j=i+1}^N\frac{d(j) - d(i)}{j-i}$$<h3 id=bendersky-2012>Bendersky 2012</h3><p><cite> “A Dictionary of Wisdom and Wit: Learning to Extract Quotable Phrases” Bendersky et al (2012) </cite><ul><li>( Most closely resembles the idea I had in mind)<li>Establishes features used in subsequent works by other people<li>Basic pipeline involves,<li>Take a book, feed sentences to Naive Bayes filter that uses log likelihood to reject most of the “unquotable” text.<li>The naive Bayes filter is trained on external sets of quotes independent of the quotes from the book<li>The output set of sentences from the NB filter is then fed into a classifier.<li>Used a quotable language model involving selected lexical, POS and punctuation features.<li>The model is a unigram model with sentence likelihood<li>Paper strongly focuses on what makes quotable phrase quotable<li>Naive Bayes filter <ul><li>Reduces noise (around 60% of text fed to it from a book is rejected)<li>High recall is important as that’s the only thing used<li>Measured by testing against a foreign set of quotable phrases to see how many pass</ul><li>Quotable Phrase detection (classifier) <ul><li>Pass a set of sentences from NB filter to single perceptron layer<li>Its labelled dataset and is trained using features mentioned above<li>They use “quotable” classified sentences and post them on /r/Quotes subreddit then average the score to see how well their classifier performs</ul></ul><h3 id=memorability-2012>Memorability 2012</h3><p><cite>“You had me at hello: How phrasing affects memorability” Cristian Danescu-Niculescu-Mizil et al (2012).</cite><ul><li>Used a corpus of movie quotes<li>Also used a corpus of common language (news article) “Brown corpus”.<li>Use two sets of features: generality and distinctiveness.<li>Distinctiveness is calculated using likelihood w.r.t the Brown corpus.<li>Generality is calculated using personal pronouns, indefinite articles, tense etc.<li>Performed a comparison of %age of memorable and non-memorable quotes that contain the above features<li>Use 1,2,3-gram Language Models and 1,2,3-gram with POS tagged Language models<li>Of each quote pair memorable is found 60% of the time to be distinct using their distinctiveness measure<li>Also included slogans from advertising and ad campaigns<li>Used distinctiveness metric and generalisability metric<li>Used SVM to get 60% accuracy, concluded that there is something memorable about these quotes</ul><h3 id=koto-2014>Koto 2014</h3><p><cite> “Memorable spoken quote corpora of TED public speaking” Koto et al (2014) </cite><ul><li>They crossed the domains of quotability/memorability of text with speech processing<li>Used TED talk dataset<li>Popularity of quote is estimated by the amount of “shares” it has on the website<li>Concluded that there is something that influences memorability of text using F0 metric from INTERSPEECH ‘09</ul><h3 id=koto-2015>Koto 2015</h3><p><cite> “A Study On Natural Expressive Speech: Automatic Memorable Spoken Quote Detection” Koto et al (2015) </cite><ul><li>Continuation of 2014 work, added features from Bendersky 2012 to extract quotability<li>Introduced two new POS features in quote detection<li>Dropped punctuation features as the data is speech based, added some acoustic features to improve speech processing<li>Compared Naive Bayes, Neural Networks and SVMs<li>Used Bendersky 2012’s features on the TED talk data set as baseline performance for the 3 methods<li>Naive Bayes gets them highest performance,<li>The feature set gives them 70.4% accuracy over 60% of baseline</ul><h3 id=koto-2016>Koto 2016</h3><p><cite> “Automatic Detection of Memorable Spoken Quotes” Koto et al (2016) </cite><ul><li>Extension of 2015 work, performed some analysis on best features to haveselect.</ul><h2 id=method>Method</h2><p>At its core, the given task can be formulated as a text classification problem. As such, I plan to employ the traditional methods for text classification and then tweak them for the task at hand. From current literature available literature that compares the performance of these techniques on the standard IMDb dataset, I have chosen TextCNN (using the GloVe embeddings) as focal technique that will be applied to the problem to see how it compares to the existing techniques discussed above.<p>TextCNN model was chosen because of its flexibility that allows for a lot more granular tweaking which can help increase performance for our given task.<p>The test hypothesis for this project is to check whether this model is able to match the performance of the traditional techniques in their respective data and see how it performs.<p>Traditionally all the papers listed above in the literature review employ a balanced dataset, however this is not indicative of the real-world scenario where quotable text is much rarer than non-quotable text. Hence I have decided to improve on this by testing various amounts of imbalanced datasets (imbalanced in favour of non-quotable text)<p>Also since the aim is to create a generalizable model that can be applied to a wide collection of tasks I start with balanced dataset and then see the impact of creating imbalance.<p>I test a 50-50 split, a 60-40 split, 70-30 split, 80-20 split and 90-10 split on the generated dataset.<h3 id=textcnn>TextCNN</h3><p>The TextCNN model hyperparameters were tweaked according to Zhang’s paper [<cite>“A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification” Zhang et al.</cite>] , with GloVe 6B, 300dim vectors.<p>Filter Size of 3, 4, 5 was used as it unconditionally yields the best result as shown in the paper. A batch size of 50 was used and a dropout of 0.5 was added to prevent overfitting. The model that we are using here is also double channeled.<h2 id=experiments>Experiments</h2><h3 id=data>Data</h3><p>The data used in the experiments are as follows,<ul><li>LitQuotes - around 2300 book quotes were scraped off of LitQuotes.com and paired up with non-quotes from same books available via Project Gutenberg.<li>Quotationspage.com - 5100 general quotes which are paired with random non-quote newspaper dataset.<li>Data from Memorability 2012’s paper - 2197 one-sentence memorable quotes paired with surrounding non-memorable quotes from the same movie, spoken by the same character and containing the same number of words.</ul><p>This gets us around 9,000 pairs of quotes and non-quotes.<h3 id=metrics>Metrics</h3><p>To measure the sucess of the system, I used F-1 score this was important as due to the skewing of the dataset, accuracy would be a bad indicator of performance as in a 90-10 split, a dumb classifier that predicts one label all the time would also yields a 90% accuracy which doesn’t accurately represent the fact that the classifier hasn’t learnt any useful feature from our dataset.<h3 id=baselines>Baselines</h3><p>The baseline we take is 70.4% which is the best that has been achieved on a similar dataset. Though I have augmented that data set with a substantial amounts of quotes, the paper only has 2000 or so memorable quotes, while in this project I have augmented that to around 9000. This is solely because 2000 quotes is very little for Deep Learning based methods to learn anything meaningful from.<h3 id=performance-and-results-of-the-method>Performance and Results of the method</h3><ul><li>Balanced (50-50) Dataset <ul><li>F-Score : 0.9417280643</ul><li>60-40 Dataset <ul><li>F-Score : 0.914163090129</ul><li>70-30 Dataset <ul><li>F-Score : 0.898954703833</ul><li>80-20 Dataset <ul><li>F-Score : 0.892349356804</ul><li>90-10 Dataset <ul><li>F-Score : 0.863253402463</ul></ul><p>This is a substaintial increase from the baseline, thus proving what I set out to, i.e., deep learning based methods are extremely accurate in discerning the quotability measure of a given sentence. These results support the hypotheis hence, in Future Work, I would like to see what are the additional features that the Deep Learning based methods generate that are able to capture the essence of quotability more accurately than the domain experts.<p>Note: Since the data for quotable text was limited, during the various higher-order splits, I just increased the amount of non-quotable text to dilute the quotable text data. So a 90-10 split has 74,000 total of quotable and non-quotable text.<h2 id=conclusions>Conclusions</h2><p>Though one of the main contributing factors that contributed to me choosing TextCNN was its speed and relative ease of implementation, after the experiments it is evident that the CNN is able to perform much better than human generated features for the same classifcation task. As to exactly what more information is a CNN able to extract that the linguists might have overlooked is uncertain, we can make some speculations to conclude what exactly is going on behind the scenes that justifies this performance bump. One of the biggest contributing factor that I would assume would be the GloVe word embeddings, that have a very interesting property of capturing word’s meaning numerically. This leads to the CNN understanding the implications of words in a quotable vs a non-quotable text, this combined with the ability of CNNs to auto-generate abstract representations of certain advanced grammatical structures, must have contributed in its ability to yield better results. <cite> “Modeling Interestingness with Deep Neural Networks” Gao, J., Pantel, P., Gamon, M., He, X., & Deng, L. (2014) </cite><h2 id=replicability>Replicability</h2><p>In the interest of replicability of the project I will be putting up all the relavant code up on a repository <sup class=footnote-reference id=fr-1-1><a href=#fn-1>[1]</a></sup><blockquote class=note><p class=alert-title><i class=icon></i>Note<p>This article was originally published on Dec 5, 2017 8:43 AM.</blockquote><hr><ol class=footnotes-list><li id=fn-1><p>Link to the repository: <a href=https://github.com/TheAnig/memorable-quotes>memorable-quotes</a> <a href=#fr-1-1>↩</a></p></ol></article><hr><nav id=post-nav><a class="post-nav-item post-nav-next" href=https://theanig.github.io/blog/ner-lstm-cnn-icml/> <div class=nav-arrow>Next</div> <span class=post-title>End-to-end Sequence-Labeling via Bi-directional LSTM CNNs CRF Tutorial</span> </a></nav><span class=hidden id=copy-code-text>Copy Code</span><span class=hidden id=search-index>https://theanig.github.io/search_index.en.json</span><span class=hidden id=more-matches-text>$MATCHES more matches</span></main><footer id=site-footer><nav><ul><li><a href=https://theanig.github.io/>About</a><li><a href=https://theanig.github.io/blog/>Blog</a></ul></nav><p>© TheAnig, 2025<ul id=socials><li><a rel=" me" href=https://discordapp.com/users/263024043391713281 title=Discord> <i style="--icon:url(&#34data:image/svg+xml,%3Csvg role='img' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Ctitle%3EDiscord%3C/title%3E%3Cpath d='M20.317 4.3698a19.7913 19.7913 0 00-4.8851-1.5152.0741.0741 0 00-.0785.0371c-.211.3753-.4447.8648-.6083 1.2495-1.8447-.2762-3.68-.2762-5.4868 0-.1636-.3933-.4058-.8742-.6177-1.2495a.077.077 0 00-.0785-.037 19.7363 19.7363 0 00-4.8852 1.515.0699.0699 0 00-.0321.0277C.5334 9.0458-.319 13.5799.0992 18.0578a.0824.0824 0 00.0312.0561c2.0528 1.5076 4.0413 2.4228 5.9929 3.0294a.0777.0777 0 00.0842-.0276c.4616-.6304.8731-1.2952 1.226-1.9942a.076.076 0 00-.0416-.1057c-.6528-.2476-1.2743-.5495-1.8722-.8923a.077.077 0 01-.0076-.1277c.1258-.0943.2517-.1923.3718-.2914a.0743.0743 0 01.0776-.0105c3.9278 1.7933 8.18 1.7933 12.0614 0a.0739.0739 0 01.0785.0095c.1202.099.246.1981.3728.2924a.077.077 0 01-.0066.1276 12.2986 12.2986 0 01-1.873.8914.0766.0766 0 00-.0407.1067c.3604.698.7719 1.3628 1.225 1.9932a.076.076 0 00.0842.0286c1.961-.6067 3.9495-1.5219 6.0023-3.0294a.077.077 0 00.0313-.0552c.5004-5.177-.8382-9.6739-3.5485-13.6604a.061.061 0 00-.0312-.0286zM8.02 15.3312c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9555-2.4189 2.157-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.9555 2.4189-2.1569 2.4189zm7.9748 0c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9554-2.4189 2.1569-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.946 2.4189-2.1568 2.4189Z'/%3E%3C/svg%3E&#34)" class=icon></i> <span>Discord</span> </a><li><a rel=" me" href=https://github.com/TheAnig title=GitHub> <i style="--icon:url(&#34data:image/svg+xml,%3Csvg role='img' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Ctitle%3EGitHub%3C/title%3E%3Cpath d='M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12'/%3E%3C/svg%3E&#34)" class=icon></i> <span>GitHub</span> </a><li><a rel=" me" href=https://www.instagram.com/theanig_gattokun/ title=Instagram> <i style="--icon:url(&#34data:image/svg+xml,%3Csvg role='img' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Ctitle%3EInstagram%3C/title%3E%3Cpath d='M7.0301.084c-1.2768.0602-2.1487.264-2.911.5634-.7888.3075-1.4575.72-2.1228 1.3877-.6652.6677-1.075 1.3368-1.3802 2.127-.2954.7638-.4956 1.6365-.552 2.914-.0564 1.2775-.0689 1.6882-.0626 4.947.0062 3.2586.0206 3.6671.0825 4.9473.061 1.2765.264 2.1482.5635 2.9107.308.7889.72 1.4573 1.388 2.1228.6679.6655 1.3365 1.0743 2.1285 1.38.7632.295 1.6361.4961 2.9134.552 1.2773.056 1.6884.069 4.9462.0627 3.2578-.0062 3.668-.0207 4.9478-.0814 1.28-.0607 2.147-.2652 2.9098-.5633.7889-.3086 1.4578-.72 2.1228-1.3881.665-.6682 1.0745-1.3378 1.3795-2.1284.2957-.7632.4966-1.636.552-2.9124.056-1.2809.0692-1.6898.063-4.948-.0063-3.2583-.021-3.6668-.0817-4.9465-.0607-1.2797-.264-2.1487-.5633-2.9117-.3084-.7889-.72-1.4568-1.3876-2.1228C21.2982 1.33 20.628.9208 19.8378.6165 19.074.321 18.2017.1197 16.9244.0645 15.6471.0093 15.236-.005 11.977.0014 8.718.0076 8.31.0215 7.0301.0839m.1402 21.6932c-1.17-.0509-1.8053-.2453-2.2287-.408-.5606-.216-.96-.4771-1.3819-.895-.422-.4178-.6811-.8186-.9-1.378-.1644-.4234-.3624-1.058-.4171-2.228-.0595-1.2645-.072-1.6442-.079-4.848-.007-3.2037.0053-3.583.0607-4.848.05-1.169.2456-1.805.408-2.2282.216-.5613.4762-.96.895-1.3816.4188-.4217.8184-.6814 1.3783-.9003.423-.1651 1.0575-.3614 2.227-.4171 1.2655-.06 1.6447-.072 4.848-.079 3.2033-.007 3.5835.005 4.8495.0608 1.169.0508 1.8053.2445 2.228.408.5608.216.96.4754 1.3816.895.4217.4194.6816.8176.9005 1.3787.1653.4217.3617 1.056.4169 2.2263.0602 1.2655.0739 1.645.0796 4.848.0058 3.203-.0055 3.5834-.061 4.848-.051 1.17-.245 1.8055-.408 2.2294-.216.5604-.4763.96-.8954 1.3814-.419.4215-.8181.6811-1.3783.9-.4224.1649-1.0577.3617-2.2262.4174-1.2656.0595-1.6448.072-4.8493.079-3.2045.007-3.5825-.006-4.848-.0608M16.953 5.5864A1.44 1.44 0 1 0 18.39 4.144a1.44 1.44 0 0 0-1.437 1.4424M5.8385 12.012c.0067 3.4032 2.7706 6.1557 6.173 6.1493 3.4026-.0065 6.157-2.7701 6.1506-6.1733-.0065-3.4032-2.771-6.1565-6.174-6.1498-3.403.0067-6.156 2.771-6.1496 6.1738M8 12.0077a4 4 0 1 1 4.008 3.9921A3.9996 3.9996 0 0 1 8 12.0077'/%3E%3C/svg%3E&#34)" class=icon></i> <span>Instagram</span> </a><li><a rel=" me" href=https://www.linkedin.com/in/anirudh-ganesh95/ title=LinkedIn> <i style="--icon:url(&#34data:image/svg+xml,%3Csvg role='img' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Ctitle%3ELinkedIn%3C/title%3E%3Cpath d='M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z'/%3E%3C/svg%3E&#34)" class=icon></i> <span>LinkedIn</span> </a><li><a rel=" me" href=https://steamcommunity.com/id/TheAnig/ title=Steam> <i style="--icon:url(&#34data:image/svg+xml,%3Csvg role='img' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Ctitle%3ESteam%3C/title%3E%3Cpath d='M11.979 0C5.678 0 .511 4.86.022 11.037l6.432 2.658c.545-.371 1.203-.59 1.912-.59.063 0 .125.004.188.006l2.861-4.142V8.91c0-2.495 2.028-4.524 4.524-4.524 2.494 0 4.524 2.031 4.524 4.527s-2.03 4.525-4.524 4.525h-.105l-4.076 2.911c0 .052.004.105.004.159 0 1.875-1.515 3.396-3.39 3.396-1.635 0-3.016-1.173-3.331-2.727L.436 15.27C1.862 20.307 6.486 24 11.979 24c6.627 0 11.999-5.373 11.999-12S18.605 0 11.979 0zM7.54 18.21l-1.473-.61c.262.543.714.999 1.314 1.25 1.297.539 2.793-.076 3.332-1.375.263-.63.264-1.319.005-1.949s-.75-1.121-1.377-1.383c-.624-.26-1.29-.249-1.878-.03l1.523.63c.956.4 1.409 1.5 1.009 2.455-.397.957-1.497 1.41-2.454 1.012H7.54zm11.415-9.303c0-1.662-1.353-3.015-3.015-3.015-1.665 0-3.015 1.353-3.015 3.015 0 1.665 1.35 3.015 3.015 3.015 1.663 0 3.015-1.35 3.015-3.015zm-5.273-.005c0-1.252 1.013-2.266 2.265-2.266 1.249 0 2.266 1.014 2.266 2.266 0 1.251-1.017 2.265-2.266 2.265-1.253 0-2.265-1.014-2.265-2.265z'/%3E%3C/svg%3E&#34)" class=icon></i> <span>Steam</span> </a><li><a rel=" me" href=https://t.me/TheAnig title=Telegram> <i style="--icon:url(&#34data:image/svg+xml,%3Csvg role='img' viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Ctitle%3ETelegram%3C/title%3E%3Cpath d='M11.944 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0a12 12 0 0 0-.056 0zm4.962 7.224c.1-.002.321.023.465.14a.506.506 0 0 1 .171.325c.016.093.036.306.02.472-.18 1.898-.962 6.502-1.36 8.627-.168.9-.499 1.201-.82 1.23-.696.065-1.225-.46-1.9-.902-1.056-.693-1.653-1.124-2.678-1.8-1.185-.78-.417-1.21.258-1.91.177-.184 3.247-2.977 3.307-3.23.007-.032.014-.15-.056-.212s-.174-.041-.249-.024c-.106.024-1.793 1.14-5.061 3.345-.48.33-.913.49-1.302.48-.428-.008-1.252-.241-1.865-.44-.752-.245-1.349-.374-1.297-.789.027-.216.325-.437.893-.663 3.498-1.524 5.83-2.529 6.998-3.014 3.332-1.386 4.025-1.627 4.476-1.635z'/%3E%3C/svg%3E&#34)" class=icon></i> <span>Telegram</span> </a></ul></footer>